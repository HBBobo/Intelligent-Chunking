{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Boundary Scorer Training\n\nv1.5.0 - Middle Ground (9 frozen, 0.4 dropout)\n\nTrain a neural model to predict semantic boundary scores (0-6) using XLM-RoBERTa.\n\n**Data**: 16,311 labeled boundaries from Gemini teacher\n**Model**: XLM-R-base with CORN ordinal head\n**Method**: CORN - respects ordinal structure\n**Changes in v1.5.0**:\n- Freeze 9/12 layers (3 trainable encoder layers) - middle ground\n- Dropout 0.4 - moderate regularization\n- Previous results: 6 frozen → 0.30, 11 frozen → 0.16"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/HBBobo/Intelligent-Chunking.git\n",
    "%cd Intelligent-Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q transformers torch scipy scikit-learn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (for saving models)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoModel, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add src to path for imports\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "# Import dp_chunk_document here so it's available throughout notebook\n",
    "from src.training.evaluate import dp_chunk_document\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths (from cloned repo)\n",
    "BOUNDARIES_PATH = Path('data/processed/all_training_data.jsonl')\n",
    "SENTENCES_DIR = Path('data/processed/sentences')\n",
    "\n",
    "# Verify data exists\n",
    "assert BOUNDARIES_PATH.exists(), f\"Boundaries file not found: {BOUNDARIES_PATH}\"\n",
    "assert SENTENCES_DIR.exists(), f\"Sentences dir not found: {SENTENCES_DIR}\"\n",
    "\n",
    "# Count data\n",
    "with open(BOUNDARIES_PATH) as f:\n",
    "    n_boundaries = sum(1 for _ in f)\n",
    "n_docs = len(list(SENTENCES_DIR.glob('*.json')))\n",
    "\n",
    "print(f'Boundaries: {n_boundaries}')\n",
    "print(f'Documents: {n_docs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import from our training module\n",
    "from src.training.dataset import BoundaryDataset, get_doc_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configuration - v1.5.0 MIDDLE GROUND\nMODEL_NAME = 'xlm-roberta-base'\nCONTEXT_SIZE = 5\nMAX_LENGTH = 512\nBATCH_SIZE = 16\nLEARNING_RATE = 2e-5\nEPOCHS = 30  # Extended - will use early stopping\nSEED = 42\n\n# Model parameters - MIDDLE GROUND\nFREEZE_LAYERS = 9   # 3 trainable encoder layers (was 6→0.30, 11→0.16)\nDROPOUT = 0.4       # Moderate dropout (was 0.3, then 0.5)\nNUM_CLASSES = 7     # Scores 0-6\n\n# CORN Ordinal Regression\nUSE_ORDINAL = True  # Use CORN instead of classification\n\n# Early stopping\nEARLY_STOPPING_PATIENCE = 5\n\n# Undersampling - keep balanced dataset\nUSE_UNDERSAMPLING = True\nUNDERSAMPLE_RATIO = 0.4\n\n# Set seeds for reproducibility\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n\nprint(f\"v1.5.0 - Middle Ground\")\nprint(f\"=\" * 50)\nprint(f\"Previous results:\")\nprint(f\"  - FREEZE_LAYERS=6, DROPOUT=0.3 → Pearson ~0.30\")\nprint(f\"  - FREEZE_LAYERS=11, DROPOUT=0.5 → Pearson ~0.16\")\nprint(f\"=\" * 50)\nprint(f\"This run:\")\nprint(f\"  - FREEZE_LAYERS: {FREEZE_LAYERS} (3 trainable layers)\")\nprint(f\"  - DROPOUT: {DROPOUT}\")\nprint(f\"  - USE_ORDINAL: {USE_ORDINAL} (CORN)\")\nprint(f\"  - EPOCHS: {EPOCHS} (patience={EARLY_STOPPING_PATIENCE})\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Split documents into train/val/test\n",
    "train_ids, val_ids, test_ids = get_doc_splits(SENTENCES_DIR, seed=SEED)\n",
    "print(f'Train: {len(train_ids)} docs, Val: {len(val_ids)} docs, Test: {len(test_ids)} docs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create datasets\ntrain_dataset = BoundaryDataset(\n    BOUNDARIES_PATH, SENTENCES_DIR, tokenizer,\n    context_size=CONTEXT_SIZE, doc_ids=set(train_ids)\n)\nval_dataset = BoundaryDataset(\n    BOUNDARIES_PATH, SENTENCES_DIR, tokenizer,\n    context_size=CONTEXT_SIZE, doc_ids=set(val_ids)\n)\ntest_dataset = BoundaryDataset(\n    BOUNDARIES_PATH, SENTENCES_DIR, tokenizer,\n    context_size=CONTEXT_SIZE, doc_ids=set(test_ids)\n)\n\nprint(f'Train samples: {len(train_dataset)}')\nprint(f'Val samples: {len(val_dataset)}')\nprint(f'Test samples: {len(test_dataset)}')\n\n# Show class distribution before balancing\nfrom collections import Counter\ntrain_scores = [train_dataset[i]['score'].item() for i in range(len(train_dataset))]\nscore_counts = Counter(train_scores)\nprint(f'\\nClass distribution (before balancing):')\nfor score in sorted(score_counts.keys()):\n    pct = score_counts[score] / len(train_scores) * 100\n    print(f'  Score {score}: {score_counts[score]:5d} ({pct:5.1f}%)')"
  },
  {
   "cell_type": "code",
   "source": "# Undersample majority classes to balance the dataset\ndef undersample_dataset(dataset, target_ratio=0.5):\n    \"\"\"\n    Undersample majority classes to balance the dataset.\n    \n    Args:\n        dataset: The original dataset\n        target_ratio: Target samples per class relative to median class count\n                     (0.5 means cap at 2x median count)\n    \n    Returns:\n        Subset of dataset with balanced classes\n    \"\"\"\n    # Get all scores\n    scores = [dataset[i]['score'].item() for i in range(len(dataset))]\n    \n    # Group indices by score\n    score_indices = {}\n    for idx, score in enumerate(scores):\n        if score not in score_indices:\n            score_indices[score] = []\n        score_indices[score].append(idx)\n    \n    # Find median count\n    counts = [len(v) for v in score_indices.values()]\n    median_count = sorted(counts)[len(counts) // 2]\n    target_count = int(median_count / target_ratio)\n    \n    print(f'Undersampling: median count = {median_count}, target cap = {target_count}')\n    \n    # Undersample classes above target\n    balanced_indices = []\n    for score in sorted(score_indices.keys()):\n        indices = score_indices[score]\n        if len(indices) > target_count:\n            sampled = random.sample(indices, target_count)\n            balanced_indices.extend(sampled)\n            print(f'  Score {score}: {len(indices)} → {target_count} (undersampled)')\n        else:\n            balanced_indices.extend(indices)\n            print(f'  Score {score}: {len(indices)} (kept all)')\n    \n    random.shuffle(balanced_indices)\n    return torch.utils.data.Subset(dataset, balanced_indices)\n\n# Apply undersampling if enabled\nif USE_UNDERSAMPLING:\n    print(f'\\nApplying undersampling with ratio={UNDERSAMPLE_RATIO}...')\n    train_dataset_balanced = undersample_dataset(train_dataset, target_ratio=UNDERSAMPLE_RATIO)\n    print(f'\\nBalanced train size: {len(train_dataset_balanced)} (was {len(train_dataset)})')\nelse:\n    train_dataset_balanced = train_dataset\n    print(f'\\nUsing full train dataset: {len(train_dataset)}')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Create data loaders (use balanced train dataset)\ntrain_loader = DataLoader(train_dataset_balanced, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n\nprint(f'Train batches: {len(train_loader)}')\nprint(f'Val batches: {len(val_loader)}')\nprint(f'Test batches: {len(test_loader)}')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model from our training module\n",
    "from src.training.model import BoundaryScorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize model with CORN ordinal head\nmodel = BoundaryScorer(\n    MODEL_NAME,\n    freeze_layers=FREEZE_LAYERS,\n    dropout=DROPOUT,\n    num_classes=NUM_CLASSES,\n    ordinal=USE_ORDINAL  # NEW: enables CORN mode\n)\nmodel = model.to(device)\n\n# Count parameters\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f'Total parameters: {total_params:,}')\nprint(f'Trainable parameters: {trainable_params:,}')\nprint(f'Frozen layers: {FREEZE_LAYERS}/12')\nprint(f'Ordinal mode: {model.ordinal}')\nprint(f'Output dimension: {NUM_CLASSES - 1 if USE_ORDINAL else NUM_CLASSES}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import evaluation function\n",
    "from src.training.evaluate import evaluate as evaluate_model_fn\n",
    "\n",
    "def evaluate_model(model, loader):\n",
    "    \"\"\"Wrapper for our evaluate function.\"\"\"\n",
    "    metrics = evaluate_model_fn(model, loader, device)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup loss function and optimizer\nfrom src.training.trainer import CORNLoss, FocalLoss, compute_class_weights\n\n# Setup optimizer\noptimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n\ntotal_steps = len(train_loader) * EPOCHS\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=int(total_steps * 0.1),\n    num_training_steps=total_steps\n)\n\n# Use CORN loss for ordinal regression\nif USE_ORDINAL:\n    criterion = CORNLoss(num_classes=NUM_CLASSES)\n    print(f'Using CORNLoss (ordinal regression)')\n    print(f'  - Converts to {NUM_CLASSES - 1} binary tasks: \"Is score > k?\"')\n    print(f'  - Respects ordinal structure: 2→3 error < 0→3 error')\nelse:\n    class_weights = compute_class_weights(BOUNDARIES_PATH, NUM_CLASSES)\n    criterion = FocalLoss(alpha=class_weights.to(device), gamma=2.0)\n    print(f'Using FocalLoss (classification)')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Training loop with early stopping and model checkpointing\nfrom src.training.trainer import corn_expected_value\n\n# Create checkpoint directory\nCHECKPOINT_DIR = '/content/drive/MyDrive/ChunkingNN/checkpoints'\n!mkdir -p \"{CHECKPOINT_DIR}\"\n\nhistory = {'train_loss': [], 'val_loss': [], 'val_pearson': []}\nbest_val_pearson = -float('inf')\nbest_val_loss = float('inf')\nbest_epoch = 0\nepochs_without_improvement = 0\n\nprint(f\"Training with early stopping (patience={EARLY_STOPPING_PATIENCE})\")\nprint(f\"Best model will be saved to: {CHECKPOINT_DIR}/best_model.pt\")\nprint(\"=\" * 60)\n\nfor epoch in range(EPOCHS):\n    model.train()\n    train_losses = []\n\n    pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{EPOCHS}')\n    for batch in pbar:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        scores = batch['score'].to(device)\n\n        optimizer.zero_grad()\n        logits = model(input_ids, attention_mask)\n        loss = criterion(logits, scores)\n        loss.backward()\n\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        scheduler.step()\n\n        train_losses.append(loss.item())\n        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n\n    # Validation\n    model.eval()\n    val_preds, val_targets = [], []\n    with torch.no_grad():\n        for batch in val_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            logits = model(input_ids, attention_mask)\n            \n            # Get expected value predictions\n            if USE_ORDINAL:\n                pred = corn_expected_value(logits)\n            else:\n                probs = torch.softmax(logits, dim=-1)\n                classes = torch.arange(NUM_CLASSES, device=device).float()\n                pred = (probs * classes).sum(dim=-1)\n            \n            val_preds.extend(pred.cpu().numpy())\n            val_targets.extend(batch['score'].numpy())\n    \n    val_preds = np.array(val_preds)\n    val_targets = np.array(val_targets).astype(float)\n    \n    val_pearson = pearsonr(val_preds, val_targets)[0]\n    val_mse = mean_squared_error(val_targets, val_preds)\n\n    avg_train_loss = np.mean(train_losses)\n    history['train_loss'].append(avg_train_loss)\n    history['val_loss'].append(val_mse)\n    history['val_pearson'].append(val_pearson)\n\n    # Check for improvement\n    improved = val_pearson > best_val_pearson\n    if improved:\n        best_val_pearson = val_pearson\n        best_val_loss = val_mse\n        best_epoch = epoch + 1\n        epochs_without_improvement = 0\n        \n        # SAVE BEST MODEL TO DISK\n        torch.save({\n            'epoch': epoch + 1,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'val_pearson': val_pearson,\n            'val_mse': val_mse,\n        }, f'{CHECKPOINT_DIR}/best_model.pt')\n        \n        status = \"✓ NEW BEST (saved)\"\n    else:\n        epochs_without_improvement += 1\n        status = f\"no improvement ({epochs_without_improvement}/{EARLY_STOPPING_PATIENCE})\"\n\n    print(f\"\\nEpoch {epoch+1}: train_loss={avg_train_loss:.4f}, \"\n          f\"val_mse={val_mse:.4f}, val_pearson={val_pearson:.4f} - {status}\")\n\n    # Early stopping\n    if epochs_without_improvement >= EARLY_STOPPING_PATIENCE:\n        print(f\"\\n{'='*60}\")\n        print(f\"EARLY STOPPING at epoch {epoch+1}\")\n        print(f\"Best model was at epoch {best_epoch} with val_pearson={best_val_pearson:.4f}\")\n        print(f\"{'='*60}\")\n        break\n\n# Load best model from checkpoint (PyTorch 2.6+ compatibility)\nprint(f\"\\nLoading best model from epoch {best_epoch}...\")\ncheckpoint = torch.load(f'{CHECKPOINT_DIR}/best_model.pt', weights_only=False)\nmodel.load_state_dict(checkpoint['model_state_dict'])\nprint(f\"Restored best model (val_pearson={checkpoint['val_pearson']:.4f})\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(history['train_loss'], label='Train')\n",
    "axes[0].plot(history['val_loss'], label='Val')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('MSE Loss')\n",
    "axes[0].legend()\n",
    "axes[0].set_title('Loss')\n",
    "\n",
    "axes[1].plot(history['val_pearson'])\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Pearson Correlation')\n",
    "axes[1].set_title('Validation Correlation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Final evaluation on test set\nmodel.eval()\npreds, targets = [], []\n\nwith torch.no_grad():\n    for batch in test_loader:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        logits = model(input_ids, attention_mask)\n        \n        # Get expected value prediction\n        if USE_ORDINAL:\n            pred = corn_expected_value(logits)\n        else:\n            probs = torch.softmax(logits, dim=-1)\n            classes = torch.arange(NUM_CLASSES, device=device).float()\n            pred = (probs * classes).sum(dim=-1)\n        \n        preds.extend(pred.cpu().numpy())\n        targets.extend(batch['score'].numpy())\n\npreds = np.array(preds)\ntargets = np.array(targets).astype(float)\n\ntest_metrics = {\n    'pearson': pearsonr(preds, targets)[0],\n    'spearman': spearmanr(preds, targets)[0],\n    'mse': mean_squared_error(targets, preds),\n    'mae': mean_absolute_error(targets, preds)\n}\n\nprint('Test Set Results:')\nprint(f\"  Pearson correlation: {test_metrics['pearson']:.4f}\")\nprint(f\"  Spearman correlation: {test_metrics['spearman']:.4f}\")\nprint(f\"  MSE: {test_metrics['mse']:.4f}\")\nprint(f\"  MAE: {test_metrics['mae']:.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "bins = np.arange(0, 7, 0.5)\n",
    "\n",
    "axes[0].hist(targets, bins=bins, alpha=0.7, label='Teacher')\n",
    "axes[0].hist(preds, bins=bins, alpha=0.7, label='Model')\n",
    "axes[0].set_xlabel('Score')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].legend()\n",
    "axes[0].set_title('Score Distribution')\n",
    "\n",
    "# Scatter plot\n",
    "axes[1].scatter(targets, preds, alpha=0.3)\n",
    "axes[1].plot([0, 6], [0, 6], 'r--', label='Perfect')\n",
    "axes[1].set_xlabel('Teacher Score')\n",
    "axes[1].set_ylabel('Model Score')\n",
    "axes[1].set_title('Prediction vs Target')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. DP Chunking Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dp_chunk_document is already imported in Cell 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Demo on a test document with TUNABLE chunking\nfrom src.training.evaluate import dp_chunk_tunable, ChunkingParams, CHUNKING_PRESETS\n\ndemo_doc_id = test_ids[0] if test_ids else list(test_dataset.sentences.keys())[0]\ndemo_sents = test_dataset.sentences[demo_doc_id]\n\nprint(f'Document: {demo_doc_id}')\nprint(f'Sentences: {len(demo_sents)}')\n\n# Get predictions for this document\nmodel.eval()\ndemo_scores = []\n\nfor i in range(len(demo_sents) - 1):\n    left = demo_sents[max(0, i - CONTEXT_SIZE + 1):i + 1]\n    right = demo_sents[i + 1:min(len(demo_sents), i + 1 + CONTEXT_SIZE)]\n    text = ' '.join(left) + f' {tokenizer.sep_token} ' + ' '.join(right)\n\n    encoding = tokenizer(text, max_length=MAX_LENGTH, truncation=True,\n                         padding='max_length', return_tensors='pt')\n\n    with torch.no_grad():\n        logits = model(\n            encoding['input_ids'].to(device),\n            encoding['attention_mask'].to(device)\n        )\n        # Use expected value for smoother scores\n        if USE_ORDINAL:\n            pred = corn_expected_value(logits)\n        else:\n            probs = torch.softmax(logits, dim=-1)\n            classes = torch.arange(NUM_CLASSES, device=device).float()\n            pred = (probs * classes).sum(dim=-1)\n        demo_scores.append(pred.item())\n\ndemo_scores = np.array(demo_scores)\nprint(f'\\nPredicted scores (first 20): {demo_scores[:20].round(1)}')\nprint(f'Score range: {demo_scores.min():.1f} - {demo_scores.max():.1f}')\n\n# Show available presets\nprint(f'\\nAvailable presets: {list(CHUNKING_PRESETS.keys())}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different chunking presets\n",
    "print(\"=\" * 70)\n",
    "print(\"CHUNKING PRESET COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for preset_name, params in CHUNKING_PRESETS.items():\n",
    "    chunks = dp_chunk_tunable(demo_sents, demo_scores.tolist(), params=params)\n",
    "    chunk_sizes = [end - start for start, end in chunks]\n",
    "    \n",
    "    print(f'\\n{preset_name.upper()} preset:')\n",
    "    print(f'  target_chunk_size={params.target_chunk_size}, target_coherency={params.target_coherency}')\n",
    "    print(f'  → {len(chunks)} chunks, sizes: {chunk_sizes[:10]}{\"...\" if len(chunk_sizes) > 10 else \"\"}')\n",
    "    print(f'  → avg size: {np.mean(chunk_sizes):.1f}, min: {min(chunk_sizes)}, max: {max(chunk_sizes)}')\n",
    "\n",
    "# Use \"balanced\" preset for detailed display\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DETAILED VIEW (balanced preset)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "chunks = dp_chunk_tunable(demo_sents, demo_scores.tolist(), preset=\"balanced\")\n",
    "\n",
    "for i, (start, end) in enumerate(chunks[:5]):\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'CHUNK {i+1} (sentences {start+1}-{end})')\n",
    "    print('='*60)\n",
    "    for j in range(start, min(end, start + 5)):\n",
    "        sent = demo_sents[j][:80] + '...' if len(demo_sents[j]) > 80 else demo_sents[j]\n",
    "        print(f'  [{j+1}] {sent}')\n",
    "    if end - start > 5:\n",
    "        print(f'  ... ({end - start - 5} more sentences)')\n",
    "    if end - 1 < len(demo_scores):\n",
    "        print(f'  -- SPLIT (score: {demo_scores[end-1]:.1f}) --')\n",
    "\n",
    "if len(chunks) > 5:\n",
    "    print(f'\\n... and {len(chunks) - 5} more chunks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Save final model to Google Drive\nSAVE_PATH = '/content/drive/MyDrive/ChunkingNN/models/boundary_scorer_v1.5'\n!mkdir -p \"{SAVE_PATH}\"\n\n# Save model weights\ntorch.save(model.state_dict(), f'{SAVE_PATH}/model.pt')\n\n# Save tokenizer\ntokenizer.save_pretrained(SAVE_PATH)\n\n# Save config\nconfig = {\n    'model_name': MODEL_NAME,\n    'context_size': CONTEXT_SIZE,\n    'max_length': MAX_LENGTH,\n    'freeze_layers': FREEZE_LAYERS,\n    'dropout': DROPOUT,\n    'num_classes': NUM_CLASSES,\n    'ordinal': USE_ORDINAL,\n    'test_pearson': float(test_metrics['pearson']),\n    'test_spearman': float(test_metrics['spearman']),\n    'test_mse': float(test_metrics['mse']),\n    'best_epoch': best_epoch\n}\nwith open(f'{SAVE_PATH}/config.json', 'w') as f:\n    json.dump(config, f, indent=2)\n\nprint(f'Model saved to {SAVE_PATH}')\nprint(f'Config: {config}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load later:\n",
    "# from src.training.model import BoundaryScorer\n",
    "# model = BoundaryScorer('xlm-roberta-base', freeze_layers=6)\n",
    "# model.load_state_dict(torch.load(f'{SAVE_PATH}/model.pt'))\n",
    "# model.eval()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}