{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boundary Scorer Training\n",
    "\n",
    "v1.2.2\n",
    "\n",
    "Train a neural model to predict semantic boundary scores (0-6) using XLM-RoBERTa.\n",
    "\n",
    "**Data**: 9,773 labeled boundaries from Gemini teacher\n",
    "**Model**: XLM-R-base with classification head (7 classes)\n",
    "**Context**: ±5 sentences around each boundary\n",
    "**Loss**: Weighted CrossEntropyLoss (handles class imbalance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/HBBobo/Intelligent-Chunking.git\n",
    "%cd Intelligent-Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q transformers torch scipy scikit-learn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (for saving models)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoModel, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add src to path for imports\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "# Import dp_chunk_document here so it's available throughout notebook\n",
    "from src.training.evaluate import dp_chunk_document\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths (from cloned repo)\n",
    "BOUNDARIES_PATH = Path('data/processed/all_training_data.jsonl')\n",
    "SENTENCES_DIR = Path('data/processed/sentences')\n",
    "\n",
    "# Verify data exists\n",
    "assert BOUNDARIES_PATH.exists(), f\"Boundaries file not found: {BOUNDARIES_PATH}\"\n",
    "assert SENTENCES_DIR.exists(), f\"Sentences dir not found: {SENTENCES_DIR}\"\n",
    "\n",
    "# Count data\n",
    "with open(BOUNDARIES_PATH) as f:\n",
    "    n_boundaries = sum(1 for _ in f)\n",
    "n_docs = len(list(SENTENCES_DIR.glob('*.json')))\n",
    "\n",
    "print(f'Boundaries: {n_boundaries}')\n",
    "print(f'Documents: {n_docs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import from our training module\n",
    "from src.training.dataset import BoundaryDataset, get_doc_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configuration - UPDATED for better training\nMODEL_NAME = 'xlm-roberta-base'\nCONTEXT_SIZE = 5\nMAX_LENGTH = 512\nBATCH_SIZE = 16\nLEARNING_RATE = 3e-5  # Slightly higher for faster convergence\nEPOCHS = 8  # More epochs with early stopping\nSEED = 42\n\n# Classification parameters - KEY CHANGES TO PREVENT MEAN COLLAPSE\nFREEZE_LAYERS = 6  # Reduced from 9 to 6 - more trainable capacity\nDROPOUT = 0.2  # Slightly lower dropout\nNUM_CLASSES = 7  # Scores 0-6\n\n# Focal Loss parameters\nUSE_FOCAL_LOSS = True  # Use Focal Loss instead of CrossEntropyLoss\nFOCAL_GAMMA = 2.0  # Focusing parameter (higher = more focus on hard examples)\n\n# Set seeds for reproducibility\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n\nprint(f\"Key changes from previous run:\")\nprint(f\"  - freeze_layers: 9 → {FREEZE_LAYERS} (more trainable params)\")\nprint(f\"  - Use FocalLoss with gamma={FOCAL_GAMMA} (focuses on hard examples)\")\nprint(f\"  - Learning rate: 2e-5 → {LEARNING_RATE}\")\nprint(f\"  - Epochs: 5 → {EPOCHS}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Split documents into train/val/test\n",
    "train_ids, val_ids, test_ids = get_doc_splits(SENTENCES_DIR, seed=SEED)\n",
    "print(f'Train: {len(train_ids)} docs, Val: {len(val_ids)} docs, Test: {len(test_ids)} docs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = BoundaryDataset(\n",
    "    BOUNDARIES_PATH, SENTENCES_DIR, tokenizer,\n",
    "    context_size=CONTEXT_SIZE, doc_ids=set(train_ids)\n",
    ")\n",
    "val_dataset = BoundaryDataset(\n",
    "    BOUNDARIES_PATH, SENTENCES_DIR, tokenizer,\n",
    "    context_size=CONTEXT_SIZE, doc_ids=set(val_ids)\n",
    ")\n",
    "test_dataset = BoundaryDataset(\n",
    "    BOUNDARIES_PATH, SENTENCES_DIR, tokenizer,\n",
    "    context_size=CONTEXT_SIZE, doc_ids=set(test_ids)\n",
    ")\n",
    "\n",
    "print(f'Train samples: {len(train_dataset)}')\n",
    "print(f'Val samples: {len(val_dataset)}')\n",
    "print(f'Test samples: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model from our training module\n",
    "from src.training.model import BoundaryScorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model with classification head\n",
    "model = BoundaryScorer(\n",
    "    MODEL_NAME,\n",
    "    freeze_layers=FREEZE_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    "    num_classes=NUM_CLASSES\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Total parameters: {total_params:,}')\n",
    "print(f'Trainable parameters: {trainable_params:,}')\n",
    "print(f'Frozen layers: {FREEZE_LAYERS}/12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import evaluation function\n",
    "from src.training.evaluate import evaluate as evaluate_model_fn\n",
    "\n",
    "def evaluate_model(model, loader):\n",
    "    \"\"\"Wrapper for our evaluate function.\"\"\"\n",
    "    metrics = evaluate_model_fn(model, loader, device)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compute class weights for imbalanced data\nfrom src.training.trainer import compute_class_weights, FocalLoss\n\nclass_weights = compute_class_weights(BOUNDARIES_PATH, NUM_CLASSES)\nprint(f'Class weights: {[f\"{w:.2f}\" for w in class_weights.tolist()]}')\n\n# Setup optimizer and scheduler\noptimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n\ntotal_steps = len(train_loader) * EPOCHS\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=int(total_steps * 0.1),\n    num_training_steps=total_steps\n)\n\n# Use Focal Loss to prevent mean collapse\nif USE_FOCAL_LOSS:\n    criterion = FocalLoss(alpha=class_weights.to(device), gamma=FOCAL_GAMMA)\n    print(f'\\nUsing FocalLoss with gamma={FOCAL_GAMMA}')\n    print(f'  - Down-weights easy examples (high pt)')\n    print(f'  - Focuses on hard examples (low pt)')\nelse:\n    criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n    print(f'\\nUsing weighted CrossEntropyLoss')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "history = {'train_loss': [], 'val_loss': [], 'val_pearson': []}\n",
    "best_val_loss = float('inf')\n",
    "best_state = None\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{EPOCHS}')\n",
    "    for batch in pbar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        scores = batch['score'].to(device)  # Integer class labels\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attention_mask)  # [batch, 7]\n",
    "        loss = criterion(logits, scores)  # CrossEntropyLoss expects logits and integer labels\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "\n",
    "    # Validation\n",
    "    val_metrics = evaluate_model(model, val_loader)\n",
    "\n",
    "    avg_train_loss = np.mean(train_losses)\n",
    "    history['train_loss'].append(avg_train_loss)\n",
    "    history['val_loss'].append(val_metrics['mse'])\n",
    "    history['val_pearson'].append(val_metrics['pearson'])\n",
    "\n",
    "    print(f\"\\nEpoch {epoch+1}: train_loss={avg_train_loss:.4f}, \"\n",
    "          f\"val_mse={val_metrics['mse']:.4f}, val_pearson={val_metrics['pearson']:.4f}\")\n",
    "\n",
    "    # Save best model\n",
    "    if val_metrics['mse'] < best_val_loss:\n",
    "        best_val_loss = val_metrics['mse']\n",
    "        best_state = model.state_dict().copy()\n",
    "\n",
    "# Restore best model\n",
    "if best_state:\n",
    "    model.load_state_dict(best_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(history['train_loss'], label='Train')\n",
    "axes[0].plot(history['val_loss'], label='Val')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('MSE Loss')\n",
    "axes[0].legend()\n",
    "axes[0].set_title('Loss')\n",
    "\n",
    "axes[1].plot(history['val_pearson'])\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Pearson Correlation')\n",
    "axes[1].set_title('Validation Correlation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation on test set\n",
    "model.eval()\n",
    "preds, targets = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        \n",
    "        # Get expected value prediction (soft prediction)\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        classes = torch.arange(NUM_CLASSES, device=device).float()\n",
    "        pred = (probs * classes).sum(dim=-1)\n",
    "        \n",
    "        preds.extend(pred.cpu().numpy())\n",
    "        targets.extend(batch['score'].numpy())\n",
    "\n",
    "preds = np.array(preds)\n",
    "targets = np.array(targets).astype(float)\n",
    "\n",
    "test_metrics = {\n",
    "    'pearson': pearsonr(preds, targets)[0],\n",
    "    'spearman': spearmanr(preds, targets)[0],\n",
    "    'mse': mean_squared_error(targets, preds),\n",
    "    'mae': mean_absolute_error(targets, preds)\n",
    "}\n",
    "\n",
    "print('Test Set Results:')\n",
    "print(f\"  Pearson correlation: {test_metrics['pearson']:.4f}\")\n",
    "print(f\"  Spearman correlation: {test_metrics['spearman']:.4f}\")\n",
    "print(f\"  MSE: {test_metrics['mse']:.4f}\")\n",
    "print(f\"  MAE: {test_metrics['mae']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "bins = np.arange(0, 7, 0.5)\n",
    "\n",
    "axes[0].hist(targets, bins=bins, alpha=0.7, label='Teacher')\n",
    "axes[0].hist(preds, bins=bins, alpha=0.7, label='Model')\n",
    "axes[0].set_xlabel('Score')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].legend()\n",
    "axes[0].set_title('Score Distribution')\n",
    "\n",
    "# Scatter plot\n",
    "axes[1].scatter(targets, preds, alpha=0.3)\n",
    "axes[1].plot([0, 6], [0, 6], 'r--', label='Perfect')\n",
    "axes[1].set_xlabel('Teacher Score')\n",
    "axes[1].set_ylabel('Model Score')\n",
    "axes[1].set_title('Prediction vs Target')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. DP Chunking Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dp_chunk_document is already imported in Cell 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo on a test document with TUNABLE chunking\n",
    "from src.training.evaluate import dp_chunk_tunable, ChunkingParams, CHUNKING_PRESETS\n",
    "\n",
    "demo_doc_id = test_ids[0] if test_ids else list(test_dataset.sentences.keys())[0]\n",
    "demo_sents = test_dataset.sentences[demo_doc_id]\n",
    "\n",
    "print(f'Document: {demo_doc_id}')\n",
    "print(f'Sentences: {len(demo_sents)}')\n",
    "\n",
    "# Get predictions for this document\n",
    "model.eval()\n",
    "demo_scores = []\n",
    "\n",
    "for i in range(len(demo_sents) - 1):\n",
    "    left = demo_sents[max(0, i - CONTEXT_SIZE + 1):i + 1]\n",
    "    right = demo_sents[i + 1:min(len(demo_sents), i + 1 + CONTEXT_SIZE)]\n",
    "    text = ' '.join(left) + f' {tokenizer.sep_token} ' + ' '.join(right)\n",
    "\n",
    "    encoding = tokenizer(text, max_length=MAX_LENGTH, truncation=True,\n",
    "                         padding='max_length', return_tensors='pt')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(\n",
    "            encoding['input_ids'].to(device),\n",
    "            encoding['attention_mask'].to(device)\n",
    "        )\n",
    "        # Use expected value for smoother scores\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        classes = torch.arange(NUM_CLASSES, device=device).float()\n",
    "        pred = (probs * classes).sum(dim=-1)\n",
    "        demo_scores.append(pred.item())\n",
    "\n",
    "demo_scores = np.array(demo_scores)\n",
    "print(f'\\nPredicted scores (first 20): {demo_scores[:20].round(1)}')\n",
    "\n",
    "# Show available presets\n",
    "print(f'\\nAvailable presets: {list(CHUNKING_PRESETS.keys())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different chunking presets\n",
    "print(\"=\" * 70)\n",
    "print(\"CHUNKING PRESET COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for preset_name, params in CHUNKING_PRESETS.items():\n",
    "    chunks = dp_chunk_tunable(demo_sents, demo_scores.tolist(), params=params)\n",
    "    chunk_sizes = [end - start for start, end in chunks]\n",
    "    \n",
    "    print(f'\\n{preset_name.upper()} preset:')\n",
    "    print(f'  target_chunk_size={params.target_chunk_size}, target_coherency={params.target_coherency}')\n",
    "    print(f'  → {len(chunks)} chunks, sizes: {chunk_sizes[:10]}{\"...\" if len(chunk_sizes) > 10 else \"\"}')\n",
    "    print(f'  → avg size: {np.mean(chunk_sizes):.1f}, min: {min(chunk_sizes)}, max: {max(chunk_sizes)}')\n",
    "\n",
    "# Use \"balanced\" preset for detailed display\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DETAILED VIEW (balanced preset)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "chunks = dp_chunk_tunable(demo_sents, demo_scores.tolist(), preset=\"balanced\")\n",
    "\n",
    "for i, (start, end) in enumerate(chunks[:5]):\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'CHUNK {i+1} (sentences {start+1}-{end})')\n",
    "    print('='*60)\n",
    "    for j in range(start, min(end, start + 5)):\n",
    "        sent = demo_sents[j][:80] + '...' if len(demo_sents[j]) > 80 else demo_sents[j]\n",
    "        print(f'  [{j+1}] {sent}')\n",
    "    if end - start > 5:\n",
    "        print(f'  ... ({end - start - 5} more sentences)')\n",
    "    if end - 1 < len(demo_scores):\n",
    "        print(f'  -- SPLIT (score: {demo_scores[end-1]:.1f}) --')\n",
    "\n",
    "if len(chunks) > 5:\n",
    "    print(f'\\n... and {len(chunks) - 5} more chunks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to Google Drive\n",
    "SAVE_PATH = '/content/drive/MyDrive/ChunkingNN/models/boundary_scorer_v1'\n",
    "!mkdir -p \"{SAVE_PATH}\"\n",
    "\n",
    "# Save model weights\n",
    "torch.save(model.state_dict(), f'{SAVE_PATH}/model.pt')\n",
    "\n",
    "# Save tokenizer\n",
    "tokenizer.save_pretrained(SAVE_PATH)\n",
    "\n",
    "# Save config\n",
    "config = {\n",
    "    'model_name': MODEL_NAME,\n",
    "    'context_size': CONTEXT_SIZE,\n",
    "    'max_length': MAX_LENGTH,\n",
    "    'freeze_layers': 6,\n",
    "    'test_pearson': float(test_metrics['pearson']),\n",
    "    'test_mse': float(test_metrics['mse'])\n",
    "}\n",
    "with open(f'{SAVE_PATH}/config.json', 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(f'Model saved to {SAVE_PATH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load later:\n",
    "# from src.training.model import BoundaryScorer\n",
    "# model = BoundaryScorer('xlm-roberta-base', freeze_layers=6)\n",
    "# model.load_state_dict(torch.load(f'{SAVE_PATH}/model.pt'))\n",
    "# model.eval()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}