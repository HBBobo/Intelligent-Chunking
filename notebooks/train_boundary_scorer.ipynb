{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Boundary Scorer Training\n\nv1.8.1 - Binary + Aggressive Regularization\n\nTrain a neural model to predict semantic boundary strength using XLM-RoBERTa.\n\n**Data**: 18,044 labeled boundaries from Gemini + Claude ensemble\n**Model**: XLM-R-base with binary classification head\n**Changes in v1.8.1**:\n- **Anti-overfitting**: Freeze 10/12 layers, dropout 0.5, LR 1e-5\n- Binary classification: No-Break (0-2) vs Break (3-6)\n- Class weighting for imbalance"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/HBBobo/Intelligent-Chunking.git\n",
    "%cd Intelligent-Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q transformers torch scipy scikit-learn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (for saving models)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoModel, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add src to path for imports\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "# Import dp_chunk_document here so it's available throughout notebook\n",
    "from src.training.evaluate import dp_chunk_document\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths (from cloned repo)\n",
    "BOUNDARIES_PATH = Path('data/processed/all_training_data.jsonl')\n",
    "SENTENCES_DIR = Path('data/processed/sentences')\n",
    "\n",
    "# Verify data exists\n",
    "assert BOUNDARIES_PATH.exists(), f\"Boundaries file not found: {BOUNDARIES_PATH}\"\n",
    "assert SENTENCES_DIR.exists(), f\"Sentences dir not found: {SENTENCES_DIR}\"\n",
    "\n",
    "# Count data\n",
    "with open(BOUNDARIES_PATH) as f:\n",
    "    n_boundaries = sum(1 for _ in f)\n",
    "n_docs = len(list(SENTENCES_DIR.glob('*.json')))\n",
    "\n",
    "print(f'Boundaries: {n_boundaries}')\n",
    "print(f'Documents: {n_docs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import from our training module\n",
    "from src.training.dataset import BoundaryDataset, get_doc_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configuration - v1.8.1 BINARY + AGGRESSIVE REGULARIZATION\nMODEL_NAME = 'xlm-roberta-base'\nCONTEXT_SIZE = 8    # Larger context (was 5)\nMAX_LENGTH = 512\nBATCH_SIZE = 16\nLEARNING_RATE = 1e-5  # Reduced from 2e-5 to prevent overfitting\nEPOCHS = 30\nSEED = 42\n\n# Model parameters - AGGRESSIVE REGULARIZATION\nFREEZE_LAYERS = 10  # Freeze 10/12 layers (was 6) - only train last 2\nDROPOUT = 0.5       # Increased from 0.3\nNUM_CLASSES = 2     # Binary: No-Break (0) vs Break (1)\n\n# Standard classification\nUSE_ORDINAL = False\n\n# Class weighting to handle imbalance (~79% No-Break, ~21% Break)\nUSE_CLASS_WEIGHTS = True\n\n# Early stopping\nEARLY_STOPPING_PATIENCE = 5\n\n# Undersampling - disabled since we're using class weights instead\nUSE_UNDERSAMPLING = False\n\n# Score mapping: 7-class to binary\n# No-Break (0): scores 0, 1, 2 - weak/no boundary, don't split\n# Break (1): scores 3, 4, 5, 6 - meaningful boundary, split here\nSCORE_MAP = {0: 0, 1: 0, 2: 0, 3: 1, 4: 1, 5: 1, 6: 1}\n\n# Set seeds\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n\nprint(f\"v1.8.1 - Binary + Aggressive Regularization\")\nprint(f\"=\" * 50)\nprint(f\"Anti-overfitting changes:\")\nprint(f\"  - Freeze 10/12 layers (was 6) - only train last 2\")\nprint(f\"  - Dropout 0.5 (was 0.3)\")\nprint(f\"  - Learning rate 1e-5 (was 2e-5)\")\nprint(f\"=\" * 50)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Split documents into train/val/test\n",
    "train_ids, val_ids, test_ids = get_doc_splits(SENTENCES_DIR, seed=SEED)\n",
    "print(f'Train: {len(train_ids)} docs, Val: {len(val_ids)} docs, Test: {len(test_ids)} docs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create datasets with score remapping\nclass RemappedDataset(torch.utils.data.Dataset):\n    \"\"\"Wrapper that remaps 7-class scores to binary.\"\"\"\n    def __init__(self, base_dataset, score_map):\n        self.base = base_dataset\n        self.score_map = score_map\n    \n    def __len__(self):\n        return len(self.base)\n    \n    def __getitem__(self, idx):\n        item = self.base[idx]\n        original_score = item['score'].item()\n        remapped_score = self.score_map[original_score]\n        return {\n            'input_ids': item['input_ids'],\n            'attention_mask': item['attention_mask'],\n            'score': torch.tensor(remapped_score, dtype=torch.long)\n        }\n\n# Create base datasets\ntrain_dataset_base = BoundaryDataset(\n    BOUNDARIES_PATH, SENTENCES_DIR, tokenizer,\n    context_size=CONTEXT_SIZE, doc_ids=set(train_ids)\n)\nval_dataset_base = BoundaryDataset(\n    BOUNDARIES_PATH, SENTENCES_DIR, tokenizer,\n    context_size=CONTEXT_SIZE, doc_ids=set(val_ids)\n)\ntest_dataset_base = BoundaryDataset(\n    BOUNDARIES_PATH, SENTENCES_DIR, tokenizer,\n    context_size=CONTEXT_SIZE, doc_ids=set(test_ids)\n)\n\n# Wrap with score remapping\ntrain_dataset = RemappedDataset(train_dataset_base, SCORE_MAP)\nval_dataset = RemappedDataset(val_dataset_base, SCORE_MAP)\ntest_dataset = RemappedDataset(test_dataset_base, SCORE_MAP)\n\n# Keep reference to sentences for DP chunking demo\ntest_dataset.sentences = test_dataset_base.sentences\n\nprint(f'Train samples: {len(train_dataset)}')\nprint(f'Val samples: {len(val_dataset)}')\nprint(f'Test samples: {len(test_dataset)}')\n\n# Show binary distribution\nfrom collections import Counter\ntrain_scores = [train_dataset[i]['score'].item() for i in range(min(1000, len(train_dataset)))]\nscore_counts = Counter(train_scores)\nclass_names = {0: 'No-Break (0-2)', 1: 'Break (3-6)'}\nprint(f'\\nBinary class distribution (sample):')\nfor cls in sorted(score_counts.keys()):\n    pct = score_counts[cls] / len(train_scores) * 100\n    print(f'  {class_names[cls]}: {score_counts[cls]:5d} ({pct:5.1f}%)')"
  },
  {
   "cell_type": "code",
   "source": "# Undersample majority classes to balance the dataset\ndef undersample_dataset(dataset, target_ratio=0.5):\n    \"\"\"\n    Undersample majority classes to balance the dataset.\n    \n    Args:\n        dataset: The original dataset\n        target_ratio: Target samples per class relative to median class count\n                     (0.5 means cap at 2x median count)\n    \n    Returns:\n        Subset of dataset with balanced classes\n    \"\"\"\n    # Get all scores\n    scores = [dataset[i]['score'].item() for i in range(len(dataset))]\n    \n    # Group indices by score\n    score_indices = {}\n    for idx, score in enumerate(scores):\n        if score not in score_indices:\n            score_indices[score] = []\n        score_indices[score].append(idx)\n    \n    # Find median count\n    counts = [len(v) for v in score_indices.values()]\n    median_count = sorted(counts)[len(counts) // 2]\n    target_count = int(median_count / target_ratio)\n    \n    print(f'Undersampling: median count = {median_count}, target cap = {target_count}')\n    \n    # Undersample classes above target\n    balanced_indices = []\n    for score in sorted(score_indices.keys()):\n        indices = score_indices[score]\n        if len(indices) > target_count:\n            sampled = random.sample(indices, target_count)\n            balanced_indices.extend(sampled)\n            print(f'  Score {score}: {len(indices)} → {target_count} (undersampled)')\n        else:\n            balanced_indices.extend(indices)\n            print(f'  Score {score}: {len(indices)} (kept all)')\n    \n    random.shuffle(balanced_indices)\n    return torch.utils.data.Subset(dataset, balanced_indices)\n\n# Apply undersampling if enabled\nif USE_UNDERSAMPLING:\n    print(f'\\nApplying undersampling with ratio={UNDERSAMPLE_RATIO}...')\n    train_dataset_balanced = undersample_dataset(train_dataset, target_ratio=UNDERSAMPLE_RATIO)\n    print(f'\\nBalanced train size: {len(train_dataset_balanced)} (was {len(train_dataset)})')\nelse:\n    train_dataset_balanced = train_dataset\n    print(f'\\nUsing full train dataset: {len(train_dataset)}')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Create data loaders (use balanced train dataset)\ntrain_loader = DataLoader(train_dataset_balanced, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n\nprint(f'Train batches: {len(train_loader)}')\nprint(f'Val batches: {len(val_loader)}')\nprint(f'Test batches: {len(test_loader)}')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model from our training module\n",
    "from src.training.model import BoundaryScorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize model with CORN ordinal head\nmodel = BoundaryScorer(\n    MODEL_NAME,\n    freeze_layers=FREEZE_LAYERS,\n    dropout=DROPOUT,\n    num_classes=NUM_CLASSES,\n    ordinal=USE_ORDINAL  # NEW: enables CORN mode\n)\nmodel = model.to(device)\n\n# Count parameters\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f'Total parameters: {total_params:,}')\nprint(f'Trainable parameters: {trainable_params:,}')\nprint(f'Frozen layers: {FREEZE_LAYERS}/12')\nprint(f'Ordinal mode: {model.ordinal}')\nprint(f'Output dimension: {NUM_CLASSES - 1 if USE_ORDINAL else NUM_CLASSES}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import evaluation function\n",
    "from src.training.evaluate import evaluate as evaluate_model_fn\n",
    "\n",
    "def evaluate_model(model, loader):\n",
    "    \"\"\"Wrapper for our evaluate function.\"\"\"\n",
    "    metrics = evaluate_model_fn(model, loader, device)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup loss function and optimizer\nfrom src.training.trainer import CORNLoss, FocalLoss, compute_class_weights\n\n# Compute class weights from training data (for binary)\nif USE_CLASS_WEIGHTS:\n    print(\"Computing class weights from training data...\")\n    # Count classes in training set\n    class_counts = {0: 0, 1: 0}\n    for i in range(len(train_dataset)):\n        score = train_dataset[i]['score'].item()\n        class_counts[score] += 1\n    \n    total = sum(class_counts.values())\n    # Inverse frequency weighting: weight = total / (num_classes * count)\n    class_weights = torch.tensor([\n        total / (NUM_CLASSES * class_counts[c]) for c in range(NUM_CLASSES)\n    ], dtype=torch.float32)\n    \n    print(f\"Class distribution:\")\n    class_labels = ['No-Break', 'Break']\n    for c in range(NUM_CLASSES):\n        pct = class_counts[c] / total * 100\n        print(f\"  {class_labels[c]}: {class_counts[c]:5d} ({pct:5.1f}%) → weight {class_weights[c]:.2f}\")\n    \n    class_weights = class_weights.to(device)\nelse:\n    class_weights = None\n\n# Setup optimizer\noptimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n\ntotal_steps = len(train_loader) * EPOCHS\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=int(total_steps * 0.1),\n    num_training_steps=total_steps\n)\n\n# Use weighted CrossEntropyLoss for binary classification\ncriterion = nn.CrossEntropyLoss(weight=class_weights)\nprint(f'\\nUsing weighted CrossEntropyLoss (binary classification)')\nprint(f'  Classes: No-Break (0-2), Break (3-6)')\nif class_weights is not None:\n    print(f'  Weights: {class_weights.cpu().tolist()}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Training loop with early stopping and model checkpointing\nfrom src.training.trainer import corn_expected_value\n\n# Create checkpoint directory\nCHECKPOINT_DIR = '/content/drive/MyDrive/ChunkingNN/checkpoints'\n!mkdir -p \"{CHECKPOINT_DIR}\"\n\nhistory = {'train_loss': [], 'val_loss': [], 'val_accuracy': []}\nbest_val_loss = float('inf')\nbest_val_accuracy = 0\nbest_epoch = 0\nepochs_without_improvement = 0\n\nprint(f\"Training with early stopping (patience={EARLY_STOPPING_PATIENCE})\")\nprint(f\"Using LOSS-BASED early stopping (more sensitive than accuracy)\")\nprint(f\"Best model will be saved to: {CHECKPOINT_DIR}/best_model.pt\")\nprint(\"=\" * 60)\n\nfor epoch in range(EPOCHS):\n    model.train()\n    train_losses = []\n\n    pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{EPOCHS}')\n    for batch in pbar:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        scores = batch['score'].to(device)\n\n        optimizer.zero_grad()\n        logits = model(input_ids, attention_mask)\n        loss = criterion(logits, scores)\n        loss.backward()\n\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        scheduler.step()\n\n        train_losses.append(loss.item())\n        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n\n    # Validation\n    model.eval()\n    val_preds, val_targets = [], []\n    val_losses = []\n    with torch.no_grad():\n        for batch in val_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            scores = batch['score'].to(device)\n            \n            logits = model(input_ids, attention_mask)\n            loss = criterion(logits, scores)\n            val_losses.append(loss.item())\n            \n            # Get predictions (argmax for classification)\n            pred = logits.argmax(dim=-1)\n            val_preds.extend(pred.cpu().numpy())\n            val_targets.extend(scores.cpu().numpy())\n    \n    val_preds = np.array(val_preds)\n    val_targets = np.array(val_targets)\n    \n    val_accuracy = (val_preds == val_targets).mean()\n    val_loss = np.mean(val_losses)\n\n    avg_train_loss = np.mean(train_losses)\n    history['train_loss'].append(avg_train_loss)\n    history['val_loss'].append(val_loss)\n    history['val_accuracy'].append(val_accuracy)\n\n    # Check for improvement using LOSS (not accuracy)\n    improved = val_loss < best_val_loss\n    if improved:\n        best_val_loss = val_loss\n        best_val_accuracy = val_accuracy\n        best_epoch = epoch + 1\n        epochs_without_improvement = 0\n        \n        # SAVE BEST MODEL TO DISK\n        torch.save({\n            'epoch': epoch + 1,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'val_accuracy': val_accuracy,\n            'val_loss': val_loss,\n        }, f'{CHECKPOINT_DIR}/best_model.pt')\n        \n        status = \"✓ NEW BEST (saved)\"\n    else:\n        epochs_without_improvement += 1\n        status = f\"no improvement ({epochs_without_improvement}/{EARLY_STOPPING_PATIENCE})\"\n\n    # Show binary prediction counts\n    pred_counts = [np.sum(val_preds == c) for c in range(NUM_CLASSES)]\n    pred_dist = f\"[NoBrk:{pred_counts[0]} Brk:{pred_counts[1]}]\"\n    \n    print(f\"\\nEpoch {epoch+1}: train_loss={avg_train_loss:.4f}, \"\n          f\"val_loss={val_loss:.4f}, val_acc={val_accuracy:.4f} {pred_dist} - {status}\")\n\n    # Early stopping\n    if epochs_without_improvement >= EARLY_STOPPING_PATIENCE:\n        print(f\"\\n{'='*60}\")\n        print(f\"EARLY STOPPING at epoch {epoch+1}\")\n        print(f\"Best model was at epoch {best_epoch} with val_loss={best_val_loss:.4f}\")\n        print(f\"{'='*60}\")\n        break\n\n# Load best model from checkpoint\nprint(f\"\\nLoading best model from epoch {best_epoch}...\")\ncheckpoint = torch.load(f'{CHECKPOINT_DIR}/best_model.pt', weights_only=False)\nmodel.load_state_dict(checkpoint['model_state_dict'])\nprint(f\"Restored best model (val_loss={checkpoint['val_loss']:.4f}, val_acc={checkpoint['val_accuracy']:.4f})\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Plot training history\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\naxes[0].plot(history['train_loss'], label='Train')\naxes[0].plot(history['val_loss'], label='Val')\naxes[0].set_xlabel('Epoch')\naxes[0].set_ylabel('Loss')\naxes[0].legend()\naxes[0].set_title('Loss')\n\naxes[1].plot(history['val_accuracy'])\naxes[1].set_xlabel('Epoch')\naxes[1].set_ylabel('Accuracy')\naxes[1].set_title('Validation Accuracy (Binary)')\naxes[1].axhline(y=0.5, color='r', linestyle='--', label='Random (50%)')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Final evaluation on test set\nmodel.eval()\npreds, targets = [], []\nprobs_all = []\n\nwith torch.no_grad():\n    for batch in test_loader:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        logits = model(input_ids, attention_mask)\n        \n        pred = logits.argmax(dim=-1)\n        probs = torch.softmax(logits, dim=-1)\n        \n        preds.extend(pred.cpu().numpy())\n        targets.extend(batch['score'].numpy())\n        probs_all.extend(probs.cpu().numpy())\n\npreds = np.array(preds)\ntargets = np.array(targets)\nprobs_all = np.array(probs_all)\n\n# Metrics\naccuracy = (preds == targets).mean()\n\n# Per-class accuracy\nclass_names = ['No-Break (0-2)', 'Break (3-6)']\nprint('Test Set Results (Binary Classification):')\nprint(f\"  Overall Accuracy: {accuracy:.4f}\")\nprint(f\"\\nPer-class accuracy:\")\nfor cls in range(NUM_CLASSES):\n    mask = targets == cls\n    if mask.sum() > 0:\n        cls_acc = (preds[mask] == targets[mask]).mean()\n        print(f\"  {class_names[cls]}: {cls_acc:.4f} ({mask.sum()} samples)\")\n\n# Precision, Recall, F1 for Break class\nfrom sklearn.metrics import precision_recall_fscore_support\nprecision, recall, f1, _ = precision_recall_fscore_support(targets, preds, average='binary', pos_label=1)\nprint(f\"\\nBreak class metrics:\")\nprint(f\"  Precision: {precision:.4f}\")\nprint(f\"  Recall: {recall:.4f}\")\nprint(f\"  F1 Score: {f1:.4f}\")\n\n# Confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(targets, preds)\nprint(f\"\\nConfusion Matrix:\")\nprint(f\"                   Pred No-Break  Pred Break\")\nfor i, name in enumerate(class_names):\n    print(f\"  True {name:15s}: {cm[i, 0]:7d}      {cm[i, 1]:7d}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualization for binary classification\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\n# Class distribution comparison\nclass_labels = ['No-Break', 'Break']\nx = np.arange(len(class_labels))\nwidth = 0.35\n\ntarget_counts = [np.sum(targets == i) for i in range(2)]\npred_counts = [np.sum(preds == i) for i in range(2)]\n\naxes[0].bar(x - width/2, target_counts, width, label='Teacher', alpha=0.8)\naxes[0].bar(x + width/2, pred_counts, width, label='Model', alpha=0.8)\naxes[0].set_xlabel('Class')\naxes[0].set_ylabel('Count')\naxes[0].set_xticks(x)\naxes[0].set_xticklabels(class_labels)\naxes[0].legend()\naxes[0].set_title('Class Distribution')\n\n# Confusion matrix heatmap\nimport matplotlib.pyplot as plt\nim = axes[1].imshow(cm, cmap='Blues')\naxes[1].set_xticks(range(2))\naxes[1].set_yticks(range(2))\naxes[1].set_xticklabels(class_labels)\naxes[1].set_yticklabels(class_labels)\naxes[1].set_xlabel('Predicted')\naxes[1].set_ylabel('True')\naxes[1].set_title('Confusion Matrix')\n\n# Add text annotations\nfor i in range(2):\n    for j in range(2):\n        axes[1].text(j, i, str(cm[i, j]), ha='center', va='center', \n                    color='white' if cm[i, j] > cm.max()/2 else 'black')\n\nplt.colorbar(im, ax=axes[1])\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. DP Chunking Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dp_chunk_document is already imported in Cell 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Demo on a test document - convert binary predictions to scores for DP chunking\nfrom src.training.evaluate import dp_chunk_tunable, ChunkingParams, CHUNKING_PRESETS\n\ndemo_doc_id = test_ids[0] if test_ids else list(test_dataset.sentences.keys())[0]\ndemo_sents = test_dataset.sentences[demo_doc_id]\n\nprint(f'Document: {demo_doc_id}')\nprint(f'Sentences: {len(demo_sents)}')\n\n# Get predictions for this document\nmodel.eval()\ndemo_scores = []\n\n# Map binary predictions to approximate scores for DP chunking\n# No-Break (0) → 1 (low score, don't split)\n# Break (1) → 5 (high score, split here)\nCLASS_TO_SCORE = {0: 1.0, 1: 5.0}\n\nfor i in range(len(demo_sents) - 1):\n    left = demo_sents[max(0, i - CONTEXT_SIZE + 1):i + 1]\n    right = demo_sents[i + 1:min(len(demo_sents), i + 1 + CONTEXT_SIZE)]\n    text = ' '.join(left) + f' {tokenizer.sep_token} ' + ' '.join(right)\n\n    encoding = tokenizer(text, max_length=MAX_LENGTH, truncation=True,\n                         padding='max_length', return_tensors='pt')\n\n    with torch.no_grad():\n        logits = model(\n            encoding['input_ids'].to(device),\n            encoding['attention_mask'].to(device)\n        )\n        # Get class prediction\n        pred_class = logits.argmax(dim=-1).item()\n        # Convert to score for DP chunking\n        demo_scores.append(CLASS_TO_SCORE[pred_class])\n\ndemo_scores = np.array(demo_scores)\nprint(f'\\nBinary predictions mapped to scores (first 20): {demo_scores[:20].astype(int)}')\nprint(f'Unique values: {sorted(set(demo_scores))}')\n\n# Show class distribution\nno_break_count = np.sum(demo_scores == 1.0)\nbreak_count = np.sum(demo_scores == 5.0)\nprint(f'Class distribution: No-Break={no_break_count}, Break={break_count}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different chunking presets\n",
    "print(\"=\" * 70)\n",
    "print(\"CHUNKING PRESET COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for preset_name, params in CHUNKING_PRESETS.items():\n",
    "    chunks = dp_chunk_tunable(demo_sents, demo_scores.tolist(), params=params)\n",
    "    chunk_sizes = [end - start for start, end in chunks]\n",
    "    \n",
    "    print(f'\\n{preset_name.upper()} preset:')\n",
    "    print(f'  target_chunk_size={params.target_chunk_size}, target_coherency={params.target_coherency}')\n",
    "    print(f'  → {len(chunks)} chunks, sizes: {chunk_sizes[:10]}{\"...\" if len(chunk_sizes) > 10 else \"\"}')\n",
    "    print(f'  → avg size: {np.mean(chunk_sizes):.1f}, min: {min(chunk_sizes)}, max: {max(chunk_sizes)}')\n",
    "\n",
    "# Use \"balanced\" preset for detailed display\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DETAILED VIEW (balanced preset)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "chunks = dp_chunk_tunable(demo_sents, demo_scores.tolist(), preset=\"balanced\")\n",
    "\n",
    "for i, (start, end) in enumerate(chunks[:5]):\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'CHUNK {i+1} (sentences {start+1}-{end})')\n",
    "    print('='*60)\n",
    "    for j in range(start, min(end, start + 5)):\n",
    "        sent = demo_sents[j][:80] + '...' if len(demo_sents[j]) > 80 else demo_sents[j]\n",
    "        print(f'  [{j+1}] {sent}')\n",
    "    if end - start > 5:\n",
    "        print(f'  ... ({end - start - 5} more sentences)')\n",
    "    if end - 1 < len(demo_scores):\n",
    "        print(f'  -- SPLIT (score: {demo_scores[end-1]:.1f}) --')\n",
    "\n",
    "if len(chunks) > 5:\n",
    "    print(f'\\n... and {len(chunks) - 5} more chunks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Save final model to Google Drive\nSAVE_PATH = '/content/drive/MyDrive/ChunkingNN/models/boundary_scorer_v1.8.1'\n!mkdir -p \"{SAVE_PATH}\"\n\n# Save model weights\ntorch.save(model.state_dict(), f'{SAVE_PATH}/model.pt')\n\n# Save tokenizer\ntokenizer.save_pretrained(SAVE_PATH)\n\n# Save config\nconfig = {\n    'model_name': MODEL_NAME,\n    'context_size': CONTEXT_SIZE,\n    'max_length': MAX_LENGTH,\n    'freeze_layers': FREEZE_LAYERS,\n    'dropout': DROPOUT,\n    'num_classes': NUM_CLASSES,\n    'ordinal': USE_ORDINAL,\n    'use_class_weights': USE_CLASS_WEIGHTS,\n    'score_map': SCORE_MAP,\n    'class_to_score': CLASS_TO_SCORE,\n    'test_accuracy': float(accuracy),\n    'test_precision': float(precision),\n    'test_recall': float(recall),\n    'test_f1': float(f1),\n    'best_epoch': best_epoch,\n    'best_val_loss': float(best_val_loss)\n}\nwith open(f'{SAVE_PATH}/config.json', 'w') as f:\n    json.dump(config, f, indent=2)\n\nprint(f'Model saved to {SAVE_PATH}')\nprint(f'Config: {config}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load later:\n",
    "# from src.training.model import BoundaryScorer\n",
    "# model = BoundaryScorer('xlm-roberta-base', freeze_layers=6)\n",
    "# model.load_state_dict(torch.load(f'{SAVE_PATH}/model.pt'))\n",
    "# model.eval()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}