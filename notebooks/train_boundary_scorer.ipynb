{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Boundary Scorer Training\n\nTrain a neural model to predict semantic boundary scores (0-6) using XLM-RoBERTa.\n\n**Data**: 9,773 labeled boundaries from Gemini teacher\n**Model**: XLM-R-base with classification head (7 classes)\n**Context**: Â±5 sentences around each boundary\n**Loss**: Weighted CrossEntropyLoss (handles class imbalance)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Clone the repository\n!git clone https://github.com/HBBobo/Intelligent-Chunking.git\n%cd Intelligent-Chunking"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q transformers torch scipy scikit-learn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (for saving models)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\nimport random\nimport sys\nfrom pathlib import Path\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW\nfrom transformers import AutoModel, AutoTokenizer, get_linear_schedule_with_warmup\nfrom scipy.stats import pearsonr, spearmanr\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\n\n# Add src to path for imports\nsys.path.insert(0, '.')\n\n# Import dp_chunk_document here so it's available throughout notebook\nfrom src.training.evaluate import dp_chunk_document\n\n# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Using device: {device}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Data paths (from cloned repo)\nBOUNDARIES_PATH = Path('data/processed/all_training_data.jsonl')\nSENTENCES_DIR = Path('data/processed/sentences')\n\n# Verify data exists\nassert BOUNDARIES_PATH.exists(), f\"Boundaries file not found: {BOUNDARIES_PATH}\"\nassert SENTENCES_DIR.exists(), f\"Sentences dir not found: {SENTENCES_DIR}\"\n\n# Count data\nwith open(BOUNDARIES_PATH) as f:\n    n_boundaries = sum(1 for _ in f)\nn_docs = len(list(SENTENCES_DIR.glob('*.json')))\n\nprint(f'Boundaries: {n_boundaries}')\nprint(f'Documents: {n_docs}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import from our training module\n",
    "from src.training.dataset import BoundaryDataset, get_doc_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configuration\nMODEL_NAME = 'xlm-roberta-base'\nCONTEXT_SIZE = 5\nMAX_LENGTH = 512\nBATCH_SIZE = 16\nLEARNING_RATE = 2e-5  # Reduced for stability\nEPOCHS = 5\nSEED = 42\n\n# Classification parameters\nFREEZE_LAYERS = 9  # Freeze 9 of 12 layers to reduce overfitting\nDROPOUT = 0.3  # Higher dropout for regularization\nNUM_CLASSES = 7  # Scores 0-6\n\n# Set seeds for reproducibility\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Split documents into train/val/test\n",
    "train_ids, val_ids, test_ids = get_doc_splits(SENTENCES_DIR, seed=SEED)\n",
    "print(f'Train: {len(train_ids)} docs, Val: {len(val_ids)} docs, Test: {len(test_ids)} docs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = BoundaryDataset(\n",
    "    BOUNDARIES_PATH, SENTENCES_DIR, tokenizer,\n",
    "    context_size=CONTEXT_SIZE, doc_ids=set(train_ids)\n",
    ")\n",
    "val_dataset = BoundaryDataset(\n",
    "    BOUNDARIES_PATH, SENTENCES_DIR, tokenizer,\n",
    "    context_size=CONTEXT_SIZE, doc_ids=set(val_ids)\n",
    ")\n",
    "test_dataset = BoundaryDataset(\n",
    "    BOUNDARIES_PATH, SENTENCES_DIR, tokenizer,\n",
    "    context_size=CONTEXT_SIZE, doc_ids=set(test_ids)\n",
    ")\n",
    "\n",
    "print(f'Train samples: {len(train_dataset)}')\n",
    "print(f'Val samples: {len(val_dataset)}')\n",
    "print(f'Test samples: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model from our training module\n",
    "from src.training.model import BoundaryScorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize model with classification head\nmodel = BoundaryScorer(\n    MODEL_NAME,\n    freeze_layers=FREEZE_LAYERS,\n    dropout=DROPOUT,\n    num_classes=NUM_CLASSES\n)\nmodel = model.to(device)\n\n# Count parameters\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f'Total parameters: {total_params:,}')\nprint(f'Trainable parameters: {trainable_params:,}')\nprint(f'Frozen layers: {FREEZE_LAYERS}/12')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import evaluation function\n",
    "from src.training.evaluate import evaluate as evaluate_model_fn\n",
    "\n",
    "def evaluate_model(model, loader):\n",
    "    \"\"\"Wrapper for our evaluate function.\"\"\"\n",
    "    metrics = evaluate_model_fn(model, loader, device)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compute class weights for imbalanced data\nfrom src.training.trainer import compute_class_weights\n\nclass_weights = compute_class_weights(BOUNDARIES_PATH, NUM_CLASSES)\nprint(f'Class weights: {class_weights.tolist()}')\nprint(f'Weights normalized: high scores get ~{class_weights[5]:.1f}x more weight than score 1')\n\n# Setup optimizer and scheduler\noptimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n\ntotal_steps = len(train_loader) * EPOCHS\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=int(total_steps * 0.1),\n    num_training_steps=total_steps\n)\n\n# Weighted classification loss for 7 classes (scores 0-6)\ncriterion = nn.CrossEntropyLoss(weight=class_weights.to(device))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Training loop\nhistory = {'train_loss': [], 'val_loss': [], 'val_pearson': []}\nbest_val_loss = float('inf')\nbest_state = None\n\nfor epoch in range(EPOCHS):\n    model.train()\n    train_losses = []\n\n    pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{EPOCHS}')\n    for batch in pbar:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        scores = batch['score'].to(device)  # Integer class labels\n\n        optimizer.zero_grad()\n        logits = model(input_ids, attention_mask)  # [batch, 7]\n        loss = criterion(logits, scores)  # CrossEntropyLoss expects logits and integer labels\n        loss.backward()\n\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        scheduler.step()\n\n        train_losses.append(loss.item())\n        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n\n    # Validation\n    val_metrics = evaluate_model(model, val_loader)\n\n    avg_train_loss = np.mean(train_losses)\n    history['train_loss'].append(avg_train_loss)\n    history['val_loss'].append(val_metrics['mse'])\n    history['val_pearson'].append(val_metrics['pearson'])\n\n    print(f\"\\nEpoch {epoch+1}: train_loss={avg_train_loss:.4f}, \"\n          f\"val_mse={val_metrics['mse']:.4f}, val_pearson={val_metrics['pearson']:.4f}\")\n\n    # Save best model\n    if val_metrics['mse'] < best_val_loss:\n        best_val_loss = val_metrics['mse']\n        best_state = model.state_dict().copy()\n\n# Restore best model\nif best_state:\n    model.load_state_dict(best_state)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(history['train_loss'], label='Train')\n",
    "axes[0].plot(history['val_loss'], label='Val')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('MSE Loss')\n",
    "axes[0].legend()\n",
    "axes[0].set_title('Loss')\n",
    "\n",
    "axes[1].plot(history['val_pearson'])\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Pearson Correlation')\n",
    "axes[1].set_title('Validation Correlation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Final evaluation on test set\nmodel.eval()\npreds, targets = [], []\n\nwith torch.no_grad():\n    for batch in test_loader:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        \n        # Get expected value prediction (soft prediction)\n        logits = model(input_ids, attention_mask)\n        probs = torch.softmax(logits, dim=-1)\n        classes = torch.arange(NUM_CLASSES, device=device).float()\n        pred = (probs * classes).sum(dim=-1)\n        \n        preds.extend(pred.cpu().numpy())\n        targets.extend(batch['score'].numpy())\n\npreds = np.array(preds)\ntargets = np.array(targets).astype(float)\n\ntest_metrics = {\n    'pearson': pearsonr(preds, targets)[0],\n    'spearman': spearmanr(preds, targets)[0],\n    'mse': mean_squared_error(targets, preds),\n    'mae': mean_absolute_error(targets, preds)\n}\n\nprint('Test Set Results:')\nprint(f\"  Pearson correlation: {test_metrics['pearson']:.4f}\")\nprint(f\"  Spearman correlation: {test_metrics['spearman']:.4f}\")\nprint(f\"  MSE: {test_metrics['mse']:.4f}\")\nprint(f\"  MAE: {test_metrics['mae']:.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "bins = np.arange(0, 7, 0.5)\n",
    "\n",
    "axes[0].hist(targets, bins=bins, alpha=0.7, label='Teacher')\n",
    "axes[0].hist(preds, bins=bins, alpha=0.7, label='Model')\n",
    "axes[0].set_xlabel('Score')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].legend()\n",
    "axes[0].set_title('Score Distribution')\n",
    "\n",
    "# Scatter plot\n",
    "axes[1].scatter(targets, preds, alpha=0.3)\n",
    "axes[1].plot([0, 6], [0, 6], 'r--', label='Perfect')\n",
    "axes[1].set_xlabel('Teacher Score')\n",
    "axes[1].set_ylabel('Model Score')\n",
    "axes[1].set_title('Prediction vs Target')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. DP Chunking Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# dp_chunk_document is already imported in Cell 5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Demo on a test document\ndemo_doc_id = test_ids[0] if test_ids else list(test_dataset.sentences.keys())[0]\ndemo_sents = test_dataset.sentences[demo_doc_id]\n\nprint(f'Document: {demo_doc_id}')\nprint(f'Sentences: {len(demo_sents)}')\n\n# Get predictions for this document\nmodel.eval()\ndemo_scores = []\n\nfor i in range(len(demo_sents) - 1):\n    left = demo_sents[max(0, i - CONTEXT_SIZE + 1):i + 1]\n    right = demo_sents[i + 1:min(len(demo_sents), i + 1 + CONTEXT_SIZE)]\n    text = ' '.join(left) + f' {tokenizer.sep_token} ' + ' '.join(right)\n\n    encoding = tokenizer(text, max_length=MAX_LENGTH, truncation=True,\n                         padding='max_length', return_tensors='pt')\n\n    with torch.no_grad():\n        logits = model(\n            encoding['input_ids'].to(device),\n            encoding['attention_mask'].to(device)\n        )\n        # Use expected value for smoother scores\n        probs = torch.softmax(logits, dim=-1)\n        classes = torch.arange(NUM_CLASSES, device=device).float()\n        pred = (probs * classes).sum(dim=-1)\n        demo_scores.append(pred.item())\n\ndemo_scores = np.array(demo_scores)\nprint(f'\\nPredicted scores (first 20): {demo_scores[:20].round(1)}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run DP chunking\n",
    "chunks = dp_chunk_document(demo_sents, demo_scores.tolist())\n",
    "print(f'\\nChunks: {len(chunks)}')\n",
    "\n",
    "# Display first 3 chunks\n",
    "for i, (start, end) in enumerate(chunks[:3]):\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'CHUNK {i+1} (sentences {start+1}-{end})')\n",
    "    print('='*60)\n",
    "    for j in range(start, min(end, start + 5)):\n",
    "        sent = demo_sents[j][:80] + '...' if len(demo_sents[j]) > 80 else demo_sents[j]\n",
    "        print(f'  [{j+1}] {sent}')\n",
    "    if end - start > 5:\n",
    "        print(f'  ... ({end - start - 5} more sentences)')\n",
    "    if end - 1 < len(demo_scores):\n",
    "        print(f'  -- SPLIT (score: {demo_scores[end-1]:.1f}) --')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to Google Drive\n",
    "SAVE_PATH = '/content/drive/MyDrive/ChunkingNN/models/boundary_scorer_v1'\n",
    "!mkdir -p \"{SAVE_PATH}\"\n",
    "\n",
    "# Save model weights\n",
    "torch.save(model.state_dict(), f'{SAVE_PATH}/model.pt')\n",
    "\n",
    "# Save tokenizer\n",
    "tokenizer.save_pretrained(SAVE_PATH)\n",
    "\n",
    "# Save config\n",
    "config = {\n",
    "    'model_name': MODEL_NAME,\n",
    "    'context_size': CONTEXT_SIZE,\n",
    "    'max_length': MAX_LENGTH,\n",
    "    'freeze_layers': 6,\n",
    "    'test_pearson': float(test_metrics['pearson']),\n",
    "    'test_mse': float(test_metrics['mse'])\n",
    "}\n",
    "with open(f'{SAVE_PATH}/config.json', 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(f'Model saved to {SAVE_PATH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load later:\n",
    "# from src.training.model import BoundaryScorer\n",
    "# model = BoundaryScorer('xlm-roberta-base', freeze_layers=6)\n",
    "# model.load_state_dict(torch.load(f'{SAVE_PATH}/model.pt'))\n",
    "# model.eval()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}